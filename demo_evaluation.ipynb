{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo notebook for algorithm evaluation\n",
    "======================================\n",
    "\n",
    "The algorithm will be evaluated on three aspects\n",
    "- Correctness\n",
    "- Perception\n",
    "- Efficiency\n",
    "\n",
    "The evaluation is performed on three tasks \n",
    "- Inpainting middle\n",
    "- Super Resolution $\\times 16$\n",
    "- out painting half\n",
    "for three images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctness\n",
    "\n",
    "Here we check how well the algorithm approximates the posterior distribution $p(x | y)$\n",
    "\n",
    "For that we work on in a step where we can evaluate explicitly the posterior: **Case of Gaussian Mixture**.\n",
    "\n",
    "In this setup, we have an analytic expression of the score of the diffusion model, the transition kernels, and more precisely the posterior $p(x | y)$.\n",
    "We use the Sliced Wasserstein (SW) distance to compare the true posterior with the approximate posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import (\n",
    "    MixtureSameFamily,\n",
    "    Categorical,\n",
    "    MultivariateNormal,\n",
    ")\n",
    "\n",
    "from evaluation.gmm import generate_inverse_problem, load_gmm_epsilon_net\n",
    "\n",
    "\n",
    "dim = 2\n",
    "n_samples = 300\n",
    "n_steps = 300\n",
    "sigma = 0.1\n",
    "\n",
    "device = \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "# define the prior distribution: #  Gaussian Gaussian\n",
    "means = torch.tensor(\n",
    "    [[8 * i, 8 * j] * (dim // 2) for i in range(-2, 3) for j in range(-2, 3)], dtype=torch.float32\n",
    ")\n",
    "n_mixtures = means.shape[0]\n",
    "covs = torch.eye(dim)[None, :].repeat(n_mixtures, 1, 1)\n",
    "weights = torch.rand(n_mixtures)\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "prior = MixtureSameFamily(Categorical(weights), MultivariateNormal(means, covs))\n",
    "\n",
    "# deduce the posterior\n",
    "obs, degradation_operator, posterior = generate_inverse_problem(\n",
    "    prior, dim, sigma, A=torch.tensor([[1, 0]], dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# define inverse problem\n",
    "inverse_problem = (obs, degradation_operator, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Notice the the ``degradation_operator`` was defined to mask the y-coordinate.\n",
    "see ``A=torch.tensor([[1, 0]], dtype=torch.float32)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's instantiate the diffusion model and use it to solve the inverse problem using DPS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling.dps import dps\n",
    "\n",
    "\n",
    "# load diffusion model trained on prior\n",
    "eps_net = load_gmm_epsilon_net(prior=prior, dim=dim, n_steps=n_steps)\n",
    "\n",
    "# solve problem\n",
    "initial_noise = torch.randn((n_samples, dim), device=device)\n",
    "reconstruction = dps(initial_noise, inverse_problem, eps_net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the prior, the posterior and the DPS reconstruction to see how they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample the prior and posterior\n",
    "samples_prior = prior.sample((n_samples,))\n",
    "samples_posterior = posterior.sample((n_samples,))\n",
    "\n",
    "# init figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "arr_samples = (samples_prior, samples_posterior, reconstruction)\n",
    "arr_labels = (\"prior\", \"posterior\", \"DPS\")\n",
    "\n",
    "# plot\n",
    "for samples, label in zip(arr_samples, arr_labels):\n",
    "    ax.scatter(\n",
    "        samples[:, 0], samples[:, 1], alpha=0.5, label=label\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "fig.legend(loc=\"upper center\", ncols=len(arr_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's compute the (SW) distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.gmm import sliced_wasserstein\n",
    "\n",
    "distance = sliced_wasserstein(samples_posterior, reconstruction)\n",
    "\n",
    "print(f\"The sliced Wasserstein distance {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perception\n",
    "\n",
    "Here we assess how well the algorithm reconstruction is *perceptually* close to the ground truth.\n",
    "\n",
    "For that we use the LPIPS metric introduced in [1] which has been shown to match human judgment.\n",
    "The smaller this metric is, the better.\n",
    "\n",
    "\n",
    ".. [1] Zhang, Richard, et al. \"The unreasonable effectiveness of deep features as a perceptual metric.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve SR16 problem using DPS and compute the LPIPS metric on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils import load_epsilon_net, load_image\n",
    "from utils import load_epsilon_net\n",
    "from sampling.dps import dps\n",
    "\n",
    "device = \"cuda:0\"\n",
    "n_steps = 300\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "# load the image\n",
    "img_path = \"./material/celebahq_img/00010.jpg\"\n",
    "x_origin = load_image(img_path, device)\n",
    "\n",
    "\n",
    "# load the degradation operator\n",
    "path_operator = f\"./material/degradation_operators/sr16.pt\"\n",
    "degradation_operator = torch.load(path_operator, map_location=device)\n",
    "\n",
    "# apply degradation operator\n",
    "y = degradation_operator.H(x_origin[None])\n",
    "y = y.squeeze(0)\n",
    "\n",
    "# add noise\n",
    "sigma = 0.01\n",
    "y = y + sigma * torch.randn_like(y)\n",
    "\n",
    "# define inverse problem\n",
    "inverse_problem = (y, degradation_operator, sigma)\n",
    "\n",
    "# load model\n",
    "eps_net = load_epsilon_net(\"celebahq\", n_steps, device)\n",
    "\n",
    "# solve problem\n",
    "initial_noise = torch.randn((1, 3, 256, 256), device=device)\n",
    "reconstruction = dps(initial_noise, inverse_problem, eps_net)\n",
    "reconstruction.clamp(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "import math\n",
    "from utils import display_image\n",
    "\n",
    "\n",
    "# reshape y\n",
    "n_channels = 3\n",
    "n_pixel_per_channel = y.shape[0] // n_channels\n",
    "hight = width = int(math.sqrt(n_pixel_per_channel))\n",
    "\n",
    "y_reshaped = y.reshape(n_channels, hight, width)\n",
    "\n",
    "# init figure\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "images = (x_origin, y_reshaped, reconstruction[0])\n",
    "titles = (\"original\", \"degraded\", \"reconstruction\")\n",
    "\n",
    "# display figures\n",
    "for ax, img, title in zip(axes, images,titles):\n",
    "    display_image(img, ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.perception import LPIPS\n",
    "\n",
    "lpips =  LPIPS()\n",
    "print(f\"lpips: {lpips.score(reconstruction.clamp(-1, 1), x_origin)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency\n",
    "\n",
    "Here we measure the run time of the algorithm and the memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count time that is need for the algorithm to solve a problem.\n",
    "Let's look on the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "\n",
    "from utils import load_epsilon_net, load_image\n",
    "from utils import load_epsilon_net\n",
    "from sampling.dps import dps\n",
    "\n",
    "device = \"cuda:0\"\n",
    "n_steps = 300\n",
    "torch.set_default_device(device)\n",
    "\n",
    "\n",
    "# load the image\n",
    "img_path = \"./material/celebahq_img/00010.jpg\"\n",
    "x_origin = load_image(img_path, device)\n",
    "\n",
    "\n",
    "# load the degradation operator\n",
    "path_operator = f\"./material/degradation_operators/sr16.pt\"\n",
    "degradation_operator = torch.load(path_operator, map_location=device)\n",
    "\n",
    "# apply degradation operator\n",
    "y = degradation_operator.H(x_origin[None])\n",
    "y = y.squeeze(0)\n",
    "\n",
    "# add noise\n",
    "sigma = 0.01\n",
    "y = y + sigma * torch.randn_like(y)\n",
    "\n",
    "# define inverse problem\n",
    "inverse_problem = (y, degradation_operator, sigma)\n",
    "\n",
    "# load model\n",
    "eps_net = load_epsilon_net(\"celebahq\", n_steps, device)\n",
    "\n",
    "# solve problem\n",
    "initial_noise = torch.randn((1, 3, 256, 256), device=device)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "reconstruction = dps(initial_noise, inverse_problem, eps_net)\n",
    "finish_time = time.perf_counter()\n",
    "\n",
    "print(f\"Elapsed time: {finish_time -start_time:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is difficult to get the memory consumption of the algorithm, we will deduce it by monitoring the output of ``nvidia-smi -l`` command.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
